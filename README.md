# Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives

## ðŸ’¥News
- [2024.02.05] The arxiv link of our survey is [hear](https://arxiv.org/pdf/2402.02968.pdf).

## ðŸ“–Table of Contents
- Overview of our survey
- Common Practices on Visual Understanding Models for Road Scenes
- Advanced Visual Understanding Foundation Models for Road Scenes
- Datasets


## Citation
If you find this work useful, please consider cite:
```
@misc{luo2024delving,
      title={Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives}, 
      author={Sheng Luo and Wei Chen and Wanxin Tian and Rui Liu and Luanxuan Hou and Xiubao Zhang and Haifeng Shen and Ruiqi Wu and Shuyi Geng and Yi Zhou and Ling Shao and Yi Yang and Bojun Gao and Qun Li and Guobin Wu},
      year={2024},
      eprint={2402.02968},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## ðŸ’—Acknowledgement
This work was supported by DiDi GAIA Research Cooperation Initiative.

