## Related Surveys
- [A survey for foundation models in autonomous driving](https://arxiv.org/abs/2402.01105)
- [Applications of Large Scale Foundation Models for Autonomous Driving](https://arxiv.org/abs/2311.12144)
- [Forging vision foundation models for autonomous driving: Challenges, methodologies, and opportunities](https://arxiv.org/abs/2401.08045)
- [LLM4Drive: A Survey of Large Language Models for Autonomous Driving](https://arxiv.org/abs/2311.01043v3)
- [A survey on multimodal large language models for autonomous driving](https://arxiv.org/abs/2311.12320)
- [Towards Knowledge-driven Autonomous Driving](https://arxiv.org/abs/2312.04316)
- [Vision language models in autonomous driving and intelligent transportation systems](https://arxiv.org/abs/2310.14414)
- [End-to-end autonomous driving: Challenges and frontiers](https://arxiv.org/abs/2306.16927)

## Task-specific Models

#### From instance-level perception to global-level understanding
- [Traffic Sign Interpretation in Real Road Scene](https://arxiv.org/abs/2311.10793)
- [Unmasking Anomalies in Road-Scene Segmentation](https://arxiv.org/abs/2307.13316)
- [Contextual Object Detection with Multimodal Large Language Models](https://arxiv.org/abs/2305.18279)
- [Learning Pedestrian Group Representations for Multi-modal Trajectory Prediction](https://arxiv.org/abs/2207.09953)
#### From Closed-set Condition to Open-set Condition
- [Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving](https://arxiv.org/abs/2308.05701)
- [OVTrack: Open-Vocabulary Multiple Object Tracking](https://arxiv.org/abs/2304.08408)
- [SalienDet: A Saliency-based Feature Enhancement Algorithm for Object Detection for Autonomous Driving](https://arxiv.org/abs/2305.06940)
- [CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving](https://arxiv.org/abs/2203.07724)
- [Using Control Synthesis to Generate Corner Cases: A Case Study on Autonomous Driving](https://arxiv.org/abs/1807.09537)
#### From Single Modality to Multiple Modalities
- [CalibFormer: A Transformer-based Automatic LiDAR-Camera Calibration Network](https://arxiv.org/abs/2311.15241)
- [Revisiting Multi-modal 3D Semantic Segmentation in Real-world Autonomous Driving](https://arxiv.org/abs/2310.08826)
- [SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields](https://arxiv.org/abs/2311.15803)
- [VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework for Multi-Modal 3D Object Detection](https://arxiv.org/abs/2401.02702)
- [Multimodal Object Query Initialization for 3D Object Detection](https://arxiv.org/abs/2310.10353)
- [TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers](https://arxiv.org/abs/2203.11496)
- [Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving](https://arxiv.org/abs/2309.14491)
- [VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision](https://arxiv.org/abs/2304.03135)
- [Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving](https://arxiv.org/abs/2305.15765)
- [CPSOR-GCN: A Vehicle Trajectory Prediction Method Powered by Emotion and Cognitive Theory](https://arxiv.org/abs/2311.08086)
## Unified Multi-task Models

#### Task-specific Outputs
- [Multi-task Learning for Real-time Autonomous Driving Leveraging Task-adaptive Attention Generator](https://arxiv.org/abs/2403.03468)
- [S3M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving](https://arxiv.org/abs/2401.11414)
- [Scalable Vision-Based 3D Object Detection and Monocular Depth Estimation for Autonomous Driving](https://arxiv.org/abs/2403.02037)
- [Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving](https://arxiv.org/abs/2305.06242)
- [Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation](https://arxiv.org/abs/2402.10580)
- [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
- [You only look at once for real-time and generic multi-task](https://arxiv.org/abs/2310.01641)
- [CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation](https://arxiv.org/abs/2311.00987)
- [LiDAR-BEVMTN: Real-Time LiDAR Bird's-Eye View Multi-Task Perception Network for Autonomous Driving](https://arxiv.org/abs/2307.08850)
- [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2311.16038)

#### Unified Language Outputs
- [DRAMA: Joint Risk Localization and Captioning in Driving](https://arxiv.org/abs/2209.10767)
- [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/abs/2302.00673)
- [HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/abs/2309.05186)
- [Traffic Sign Interpretation in Real Road Scene](https://arxiv.org/abs/2311.10793)
- [Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/abs/2312.03661)
## Unified Multi-modal Models

#### LLM Functions as Sequence Modeling
- [A Language Agent for Autonomous Driving](https://arxiv.org/abs/2311.10813)
- [Empowering Autonomous Driving with Large Language Models: A Safety Perspective](https://arxiv.org/abs/2312.00812)
- [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)
- [Drive Like a Human: Rethinking Autonomous Driving with Large Language Models](https://arxiv.org/abs/2307.07162)
- [Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://arxiv.org/abs/2310.01957)
- [LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving](https://arxiv.org/abs/2310.03026)
- [DRIVEGPT4: INTERPRETABLE END-TO-END AUTONOMOUS DRIVING VIA LARGE LANGUAGE MODEL](https://arxiv.org/abs/2310.01412)
- [GPT-Driver: Learning to Drive with GPT](https://arxiv.org/abs/2310.01415)
- [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245)

#### Cross-modal Interaction in VLM
- [DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models](https://arxiv.org/abs/2402.12289)
- [Language Prompt for Autonomous Driving](https://arxiv.org/abs/2309.04379)
- [Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving](https://arxiv.org/abs/2310.02251)
- [Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models](https://arxiv.org/abs/2310.17642)
- [Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/abs/2312.03661)
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
- [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/abs/2311.05332)

## Prompting Foundation Models

#### [Textual Prompt](#llm-functions-as-sequence-modeling)

#### Visual Prompt
- [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
- [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/abs/2311.05332)
- [Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving](https://arxiv.org/abs/2310.02251)
- [Talk2Car: Taking Control of Your Self-Driving Car](https://arxiv.org/abs/1909.10838)
- [Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?](https://arxiv.org/abs/2304.10224)
- [Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2310.04456)


#### Multi-step Prompt
- [OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data](https://arxiv.org/abs/2310.13398)
- [GPT-Driver: Learning to Drive with GPT](https://arxiv.org/abs/2310.01415)
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
#### Task-specific Prompt
- [Multi-task learning with multi-query transformer for dense prediction](https://arxiv.org/abs/2205.14354)
- [Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection, Segmentation, and Depth Estimation](https://arxiv.org/abs/2304.00971)
- [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
- [Effective Adaptation in Multi-Task Co-Training for Unified Autonomous Driving](https://arxiv.org/abs/2209.08953)

#### Geometric Prompt
- [Beyond Empirical Windowing: An Attention-Based Approach for Trust Prediction in Autonomous Vehicles](https://arxiv.org/abs/2312.10209)
- [DRIVEGPT4: INTERPRETABLE END-TO-END AUTONOMOUS DRIVING VIA LARGE LANGUAGE MODEL](https://arxiv.org/abs/2310.01412)
#### Prompt Pool
- [I3DOD: Towards Incremental 3D Object Detection via Prompting](https://arxiv.org/abs/2308.12512)

## Datasets
- [Lampilot: An open benchmark dataset for autonomous driving with language model programs](https://arxiv.org/abs/2312.04372)
- [Drivelm: Driving with graph visual question answering](https://arxiv.org/abs/2312.14150)
- [Drivegpt4: Interpretable end-to-end autonomous driving via large language model](https://arxiv.org/abs/2310.01412)
- [Talk2bev: Language-enhanced bird's-eye view maps for autonomous driving](https://arxiv.org/abs/2310.02251)
- [Rank2tell: A multimodal driving dataset for joint importance ranking and reasoning](https://arxiv.org/abs/2309.06597)
- [Language prompt for autonomous driving](https://arxiv.org/abs/2309.04379)
- [Nuscenes-qa: A multi-modal visual question answering benchmark for autonomous driving scenario](https://arxiv.org/abs/2305.14836)
- [Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving](https://arxiv.org/abs/2312.09245)
- [Lmdrive: Closed-loop end-to-end driving with large language models](https://arxiv.org/abs/2312.07488)
- [Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving](https://arxiv.org/abs/2312.03661)
- [Driving with llms: Fusing object-level vector modality for explainable autonomous driving](https://arxiv.org/abs/2310.01957)
- [Referring multi-object tracking](https://arxiv.org/abs/2303.03366)
- [NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations](https://arxiv.org/abs/2312.06352)
- [Lidar-llm: Exploring the potential of large language models for 3d lidar understanding](https://arxiv.org/abs/2312.14074)
- [Drama: Joint risk localization and captioning in driving](https://arxiv.org/abs/2209.10767)
## Towards Open-world Understanding

#### Label Shift
- [DriveLM: Driving with Graph Visual Question Answering](https://arxiv.org/abs/2312.14150)
- [Semantic Anomaly Detection with Large Language Models](https://arxiv.org/abs/2305.11307)
- [TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors](https://arxiv.org/abs/2101.06557)
- [KING: Generating Safety-Critical Driving Scenarios for Robust Imitation via Kinematics Gradients](https://arxiv.org/abs/2204.13683)

#### Domain Shift
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
- [Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models](https://arxiv.org/abs/2310.17642)
- [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/abs/2311.05332)

## Efficient Transfer for Road Scenes

#### Knowledge Distillation
- [LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving](https://arxiv.org/abs/2403.08215)
- [LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization](https://arxiv.org/abs/2312.16648)
- [LidarCLIP or: How I Learned to Talk to Point Clouds](https://arxiv.org/abs/2212.06858)
- [CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP](https://arxiv.org/abs/2301.04926)
- [VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking](https://arxiv.org/abs/2303.11301)
- [RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM](https://arxiv.org/abs/2401.03907)
- [Learning to Adapt SAM for Segmenting Cross-domain Point Clouds](https://arxiv.org/abs/2310.08820)
- [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245)
#### Instant Learning
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
- [DriveLM: Driving with Graph Visual Question Answering](https://arxiv.org/abs/2312.14150)

## Continual Learning
#### Rehearsal-based methods
- [Brain-inspired domain-incremental adaptive detection for autonomous driving](https://www.frontiersin.org/articles/10.3389/fnbot.2022.916808/full)
- [An Efficient Domain-Incremental Learning Approach to Drive in All Weather Conditions](https://arxiv.org/abs/2204.08817)
#### Architecture-based methods
- [Actively Semi-Supervised Deep Rule-based Classifier Applied to Adverse Driving Scenarios](https://ieeexplore.ieee.org/document/8851842)
#### LLMs as knowledge base
- [Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles](https://arxiv.org/abs/2309.10228)
- [Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles](https://arxiv.org/abs/2310.08034)
- [DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models](https://arxiv.org/abs/2309.16292)
- [Drive Like a Human: Rethinking Autonomous Driving with Large Language Models](https://arxiv.org/abs/2307.07162)
- [Empowering Autonomous Driving with Large Language Models: A Safety Perspective](https://arxiv.org/abs/2312.00812)


## Learn to Interact
#### Interaction with human.
- [DRIVEGPT4: INTERPRETABLE END-TO-END AUTONOMOUS DRIVING VIA LARGE LANGUAGE MODEL](https://arxiv.org/abs/2310.01412)
- [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)
- [DRAMA: Joint Risk Localization and Captioning in Driving](https://arxiv.org/abs/2209.10767)
- [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/abs/2302.00673)
#### Error recovery
- [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)

#### Adaptive driving behavior
- [Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2402.13481)
- [In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment](https://arxiv.org/abs/2311.09273)
- [Situational driving anger, driving performance and allocation of visual attention](https://www.researchgate.net/profile/Yutao-Ba/publication/309475593_Situational_driving_anger_driving_performance_and_allocation_of_visual_attention/links/5bf53c8ca6fdcc3a8de66552/Situational-driving-anger-driving-performance-and-allocation-of-visual-attention.pdf)
- [CPSOR-GCN: A Vehicle Trajectory Prediction Method Powered by Emotion and Cognitive Theory](https://arxiv.org/abs/2311.08086)
## Generative Foundation Models
#### Image representation
- [Text2Street: Controllable Text-to-image Generation for Street Views](https://arxiv.org/abs/2402.04504)
- [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators)
- [GenAD: Generative End-to-End Autonomous Driving](https://arxiv.org/abs/2402.11502)
- [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/abs/2309.17080)
- [DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/abs/2309.09777)
- [MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations](https://arxiv.org/abs/2311.11762)
- [ADriver-I: A General World Model for Autonomous Driving](https://arxiv.org/abs/2311.13549)
- [TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction](https://arxiv.org/abs/2303.04116)
- [ReCoRe: Regularized Contrastive Representation Learning of World Model](https://arxiv.org/abs/2312.09056)
- [UniWorld: Autonomous Driving Pre-training via World Models](https://arxiv.org/abs/2308.07234)
- [Recurrent World Models Facilitate Policy Evolution](https://arxiv.org/abs/1809.01999)

#### Occupancy representation
- [OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework for 3D Occupancy Prediction](https://arxiv.org/abs/2403.01644)
- [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2311.16038)
## Closed-loop Driving Systems
- [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)
- [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245)

## Interpretability
- [RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model](https://arxiv.org/abs/2402.10828)
- [HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/abs/2309.05186)
- [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/abs/2302.00673)
- [DRAMA: Joint Risk Localization and Captioning in Driving](https://arxiv.org/abs/2209.10767)

- [Multi-task Learning with Attention for End-to-end Autonomous Driving](https://arxiv.org/abs/2104.10753)
- [TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving](https://arxiv.org/abs/2205.15997)

- [Explaining Autonomous Driving Actions with Visual Question Answering](https://arxiv.org/abs/2307.10408)
- [DriveLM: Driving with Graph Visual Question Answering](https://arxiv.org/abs/2312.14150)
- [Multi-Modal Fusion Transformer for End-to-End Autonomous Driving](https://arxiv.org/abs/2104.09224)
- [Hidden Biases of End-to-End Driving Models](https://arxiv.org/abs/2306.07957)

## Low-resource Condition

- [Quantized convolutional neural networks through the lens of partial differential equations](https://arxiv.org/abs/2109.00095)
- [Training a Binary Weight Object Detector by Knowledge Transfer for Autonomous Driving](https://arxiv.org/abs/1804.06332)
- [DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car](https://arxiv.org/abs/1712.08644)
- [Robust and efficient post-processing for video object detection](https://arxiv.org/abs/2009.11050)
- [Edge YOLO: Real-Time Intelligent Object Detection System Based on Edge-Cloud Cooperation in Autonomous Vehicles](https://arxiv.org/abs/2205.14942)
- [ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation](https://arxiv.org/abs/2309.05527)
- [Generalized Few-Shot 3D Object Detection of LiDAR Point Cloud for Autonomous Driving](https://arxiv.org/abs/2302.03914)
- [Few-shot 3D LiDAR Semantic Segmentation for Autonomous Driving](https://arxiv.org/abs/2302.08785)
- [Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models](https://arxiv.org/abs/2312.13763)
- [S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and Generation](https://arxiv.org/abs/2402.02112)
- [Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following](https://arxiv.org/abs/2402.06559)
- [Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents](https://arxiv.org/abs/2402.05746)
- [NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance Fields](https://arxiv.org/abs/2304.14811)
- [MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous Driving](https://arxiv.org/abs/2307.15058)
- [DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes](https://arxiv.org/abs/2312.07920)


## Embodied Driving Agent
- [Embodied Understanding of Driving Scenarios](https://arxiv.org/abs/2403.04593)
- [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)
- [RT-1: Robotics Transformer for Real-World Control at Scale](https://arxiv.org/abs/2212.06817)
- [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/abs/2307.15818)
- [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/abs/2310.08864v4)

## World Model
- [Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)](https://arxiv.org/abs/2402.16720)
- [DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation](https://arxiv.org/abs/2403.06845)
- [World Models for Autonomous Driving: An Initial Survey](https://arxiv.org/abs/2403.02622)
- [Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving](https://arxiv.org/abs/2311.17918)
- [Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)
- [World Model on Million-Length Video And Language With RingAttention](https://arxiv.org/abs/2402.08268)
- [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/abs/2309.17080)
- [DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/abs/2309.09777)
- [Voyager: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/abs/2305.16291)
- [Others](#generative-foundation-models)







