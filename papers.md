## Related Surveys
- [A survey for foundation models in autonomous driving](https://arxiv.org/abs/2402.01105)
- [Applications of Large Scale Foundation Models for Autonomous Driving](https://arxiv.org/abs/2311.12144)
- [Forging vision foundation models for autonomous driving: Challenges, methodologies, and opportunities](https://arxiv.org/abs/2401.08045)
- [LLM4Drive: A Survey of Large Language Models for Autonomous Driving](https://arxiv.org/abs/2311.01043v3)
- [A survey on multimodal large language models for autonomous driving](https://arxiv.org/abs/2311.12320)
- [Towards Knowledge-driven Autonomous Driving](https://arxiv.org/abs/2312.04316)
- [Vision language models in autonomous driving and intelligent transportation systems](https://arxiv.org/abs/2310.14414)
- [End-to-end autonomous driving: Challenges and frontiers](https://arxiv.org/abs/2306.16927)

## Task-specific Models

#### From instance-level perception to global-level understanding
- [Traffic Sign Interpretation in Real Road Scene](https://arxiv.org/abs/2311.10793)
- [Unmasking Anomalies in Road-Scene Segmentation](https://arxiv.org/abs/2307.13316)
- [Contextual Object Detection with Multimodal Large Language Models](https://arxiv.org/abs/2305.18279)
- [Learning Pedestrian Group Representations for Multi-modal Trajectory Prediction](https://arxiv.org/abs/2207.09953)
#### From Closed-set Condition to Open-set Condition
- [Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving](https://arxiv.org/abs/2308.05701)
- [OVTrack: Open-Vocabulary Multiple Object Tracking](https://arxiv.org/abs/2304.08408)
- [SalienDet: A Saliency-based Feature Enhancement Algorithm for Object Detection for Autonomous Driving](https://arxiv.org/abs/2305.06940)
- [CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving](https://arxiv.org/abs/2203.07724)
- [Using Control Synthesis to Generate Corner Cases: A Case Study on Autonomous Driving](https://arxiv.org/abs/1807.09537)
#### From Single Modality to Multiple Modalities
- [CalibFormer: A Transformer-based Automatic LiDAR-Camera Calibration Network](https://arxiv.org/abs/2311.15241)
- [Revisiting Multi-modal 3D Semantic Segmentation in Real-world Autonomous Driving](https://arxiv.org/abs/2310.08826)
- [SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields](https://arxiv.org/abs/2311.15803)
- [VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework for Multi-Modal 3D Object Detection](https://arxiv.org/abs/2401.02702)
- [Multimodal Object Query Initialization for 3D Object Detection](https://arxiv.org/abs/2310.10353)
- [TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers](https://arxiv.org/abs/2203.11496)
- [Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving](https://arxiv.org/abs/2309.14491)
- [VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision](https://arxiv.org/abs/2304.03135)
- [Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving](https://arxiv.org/abs/2305.15765)
- [CPSOR-GCN: A Vehicle Trajectory Prediction Method Powered by Emotion and Cognitive Theory](https://arxiv.org/abs/2311.08086)
## Unified Multi-task Models

#### Task-specific Outputs
- [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
- [You only look at once for real-time and generic multi-task](https://arxiv.org/abs/2310.01641)
- [CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation](https://arxiv.org/abs/2311.00987)
- [LiDAR-BEVMTN: Real-Time LiDAR Bird's-Eye View Multi-Task Perception Network for Autonomous Driving](https://arxiv.org/abs/2307.08850)
- [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2311.16038)

#### Unified Language Outputs
- [DRAMA: Joint Risk Localization and Captioning in Driving](https://arxiv.org/abs/2209.10767)
- [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/abs/2302.00673)
- [HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/abs/2309.05186)
- [Traffic Sign Interpretation in Real Road Scene](https://arxiv.org/abs/2311.10793)
- [Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/abs/2312.03661)
## Unified Multi-modal Models

#### LLM Functions as Sequence Modeling
- [A Language Agent for Autonomous Driving](https://arxiv.org/abs/2311.10813)
- [Empowering Autonomous Driving with Large Language Models: A Safety Perspective](https://arxiv.org/abs/2312.00812)
- [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)
- [Drive Like a Human: Rethinking Autonomous Driving with Large Language Models](https://arxiv.org/abs/2307.07162)
- [Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://arxiv.org/abs/2310.01957)
- [LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving](https://arxiv.org/abs/2310.03026)
- [DRIVEGPT4: INTERPRETABLE END-TO-END AUTONOMOUS DRIVING VIA LARGE LANGUAGE MODEL](https://arxiv.org/abs/2310.01412)
- [GPT-Driver: Learning to Drive with GPT](https://arxiv.org/abs/2310.01415)
- [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245)
#### Cross-modal Interaction in VLM
- [Language Prompt for Autonomous Driving](https://arxiv.org/abs/2309.04379)
- [Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving](https://arxiv.org/abs/2310.02251)
- [Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models](https://arxiv.org/abs/2310.17642)
- [Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/abs/2312.03661)
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
- [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/abs/2311.05332)

## Prompting Foundation Models

#### [Textual Prompt](#llm-functions-as-sequence-modeling)

#### Visual Prompt
- [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
- [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/abs/2311.05332)
- [Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving](https://arxiv.org/abs/2310.02251)
- [Talk2Car: Taking Control of Your Self-Driving Car](https://arxiv.org/abs/1909.10838)
- [Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?](https://arxiv.org/abs/2304.10224)
- [Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2310.04456)


#### Multi-step Prompt
- [OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data](https://arxiv.org/abs/2310.13398)
- [GPT-Driver: Learning to Drive with GPT](https://arxiv.org/abs/2310.01415)
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
#### Task-specific Prompt
- [Multi-task learning with multi-query transformer for dense prediction](https://arxiv.org/abs/2205.14354)
- [Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection, Segmentation, and Depth Estimation](https://arxiv.org/abs/2304.00971)
- [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
- [Effective Adaptation in Multi-Task Co-Training for Unified Autonomous Driving](https://arxiv.org/abs/2209.08953)

#### Geometric Prompt
- [Beyond Empirical Windowing: An Attention-Based Approach for Trust Prediction in Autonomous Vehicles](https://arxiv.org/abs/2312.10209)
- [DRIVEGPT4: INTERPRETABLE END-TO-END AUTONOMOUS DRIVING VIA LARGE LANGUAGE MODEL](https://arxiv.org/abs/2310.01412)
#### Prompt Pool
- [I3DOD: Towards Incremental 3D Object Detection via Prompting](https://arxiv.org/abs/2308.12512)
## Towards Open-world Understanding

#### Label Shift
- [DriveLM: Driving with Graph Visual Question Answering](https://arxiv.org/abs/2312.14150)
- [Semantic Anomaly Detection with Large Language Models](https://arxiv.org/abs/2305.11307)
- [TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors](https://arxiv.org/abs/2101.06557)
- [KING: Generating Safety-Critical Driving Scenarios for Robust Imitation via Kinematics Gradients](https://arxiv.org/abs/2204.13683)

#### Domain Shift
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
- [Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models](https://arxiv.org/abs/2310.17642)
- [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/abs/2311.05332)

## Efficient Transfer for Road Scenes

#### Knowledge Distillation
- [LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization](https://arxiv.org/abs/2312.16648)
- [LidarCLIP or: How I Learned to Talk to Point Clouds](https://arxiv.org/abs/2212.06858)
- [CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP](https://arxiv.org/abs/2301.04926)
- [VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking](https://arxiv.org/abs/2303.11301)
- [RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM](https://arxiv.org/abs/2401.03907)
- [Learning to Adapt SAM for Segmenting Cross-domain Point Clouds](https://arxiv.org/abs/2310.08820)
- [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245)
#### Instant Learning
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
- [DriveLM: Driving with Graph Visual Question Answering](https://arxiv.org/abs/2312.14150)

## Continual Learning
#### Rehearsal-based methods
- [Brain-inspired domain-incremental adaptive detection for autonomous driving](https://www.frontiersin.org/articles/10.3389/fnbot.2022.916808/full)
- [An Efficient Domain-Incremental Learning Approach to Drive in All Weather Conditions](https://arxiv.org/abs/2204.08817)
#### Architecture-based methods
- [Actively Semi-Supervised Deep Rule-based Classifier Applied to Adverse Driving Scenarios](https://ieeexplore.ieee.org/document/8851842)
#### LLMs as knowledge base
- [Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles](https://arxiv.org/abs/2309.10228)
- [Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles](https://arxiv.org/abs/2310.08034)
- [DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models](https://arxiv.org/abs/2309.16292)
- [Drive Like a Human: Rethinking Autonomous Driving with Large Language Models](https://arxiv.org/abs/2307.07162)
- [Empowering Autonomous Driving with Large Language Models: A Safety Perspective](https://arxiv.org/abs/2312.00812)


## Learn to Interact
#### Interaction with human.
- [DRIVEGPT4: INTERPRETABLE END-TO-END AUTONOMOUS DRIVING VIA LARGE LANGUAGE MODEL](https://arxiv.org/abs/2310.01412)
- [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)
- [DRAMA: Joint Risk Localization and Captioning in Driving](https://arxiv.org/abs/2209.10767)
- [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/abs/2302.00673)
#### Error recovery
- [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)

#### Adaptive driving behavior
- [In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment](https://arxiv.org/abs/2311.09273)
- [Situational driving anger, driving performance and allocation of visual attention](https://www.researchgate.net/profile/Yutao-Ba/publication/309475593_Situational_driving_anger_driving_performance_and_allocation_of_visual_attention/links/5bf53c8ca6fdcc3a8de66552/Situational-driving-anger-driving-performance-and-allocation-of-visual-attention.pdf)
- [CPSOR-GCN: A Vehicle Trajectory Prediction Method Powered by Emotion and Cognitive Theory](https://arxiv.org/abs/2311.08086)
## Generative Foundation Models
#### Image representation
- [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/abs/2309.17080)
- [DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/abs/2309.09777)
- [MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations](https://arxiv.org/abs/2311.11762)
- [ADriver-I: A General World Model for Autonomous Driving](https://arxiv.org/abs/2311.13549)
- [TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction](https://arxiv.org/abs/2303.04116)
- [ReCoRe: Regularized Contrastive Representation Learning of World Model](https://arxiv.org/abs/2312.09056)
- [UniWorld: Autonomous Driving Pre-training via World Models](https://arxiv.org/abs/2308.07234)
- [Recurrent World Models Facilitate Policy Evolution](https://arxiv.org/abs/1809.01999)

#### Occupancy representation
- [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2311.16038)
## Closed-loop Driving Systems
- [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)
- [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245)

## Interpretability
- [HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/abs/2309.05186)
- [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/abs/2302.00673)
- [DRAMA: Joint Risk Localization and Captioning in Driving](https://arxiv.org/abs/2209.10767)

- [Multi-task Learning with Attention for End-to-end Autonomous Driving](https://arxiv.org/abs/2104.10753)
- [TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving](https://arxiv.org/abs/2205.15997)

- [Explaining Autonomous Driving Actions with Visual Question Answering](https://arxiv.org/abs/2307.10408)
- [DriveLM: Driving with Graph Visual Question Answering](https://arxiv.org/abs/2312.14150)
- [Multi-Modal Fusion Transformer for End-to-End Autonomous Driving](https://arxiv.org/abs/2104.09224)
- [Hidden Biases of End-to-End Driving Models](https://arxiv.org/abs/2306.07957)


## Embodied Driving Agent
- [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)
- [RT-1: Robotics Transformer for Real-World Control at Scale](https://arxiv.org/abs/2212.06817)
- [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/abs/2307.15818)
- [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/abs/2310.08864v4)

## World Model
- [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/abs/2309.17080)
- [DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/abs/2309.09777)
- [Voyager: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/abs/2305.16291)







