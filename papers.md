## Related Surveys
- [A survey for foundation models in autonomous driving](https://arxiv.org/abs/2402.01105)
- [Applications of Large Scale Foundation Models for Autonomous Driving](https://arxiv.org/abs/2311.12144)
- [Forging vision foundation models for autonomous driving: Challenges, methodologies, and opportunities](https://arxiv.org/abs/2401.08045)
- [LLM4Drive: A Survey of Large Language Models for Autonomous Driving](https://arxiv.org/abs/2311.01043v3)
- [A survey on multimodal large language models for autonomous driving](https://arxiv.org/abs/2311.12320)
- [Towards Knowledge-driven Autonomous Driving](https://arxiv.org/abs/2312.04316)
- [Vision language models in autonomous driving and intelligent transportation systems](https://arxiv.org/abs/2310.14414)
- [End-to-end autonomous driving: Challenges and frontiers](https://arxiv.org/abs/2306.16927)

## Task-specific Models

#### From instance-level perception to global-level understanding
- [Traffic Sign Interpretation in Real Road Scene](https://arxiv.org/abs/2311.10793)
- [Unmasking Anomalies in Road-Scene Segmentation](https://arxiv.org/abs/2307.13316)
- [Contextual Object Detection with Multimodal Large Language Models](https://arxiv.org/abs/2305.18279)
- [Learning Pedestrian Group Representations for Multi-modal Trajectory Prediction](https://arxiv.org/abs/2207.09953)
#### From Closed-set Condition to Open-set Condition
- [Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving](https://arxiv.org/abs/2308.05701)
- [OVTrack: Open-Vocabulary Multiple Object Tracking](https://arxiv.org/abs/2304.08408)
- [SalienDet: A Saliency-based Feature Enhancement Algorithm for Object Detection for Autonomous Driving](https://arxiv.org/abs/2305.06940)
- [CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving](https://arxiv.org/abs/2203.07724)
- [Using Control Synthesis to Generate Corner Cases: A Case Study on Autonomous Driving](https://arxiv.org/abs/1807.09537)
#### From Single Modality to Multiple Modalities
- [CalibFormer: A Transformer-based Automatic LiDAR-Camera Calibration Network](https://arxiv.org/abs/2311.15241)
- [Revisiting Multi-modal 3D Semantic Segmentation in Real-world Autonomous Driving](https://arxiv.org/abs/2310.08826)
- [SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields](https://arxiv.org/abs/2311.15803)
- [VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework for Multi-Modal 3D Object Detection](https://arxiv.org/abs/2401.02702)
- [Multimodal Object Query Initialization for 3D Object Detection](https://arxiv.org/abs/2310.10353)
- [TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers](https://arxiv.org/abs/2203.11496)
- [Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving](https://arxiv.org/abs/2309.14491)
- [VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision](https://arxiv.org/abs/2304.03135)
- [Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving](https://arxiv.org/abs/2305.15765)
- [CPSOR-GCN: A Vehicle Trajectory Prediction Method Powered by Emotion and Cognitive Theory](https://arxiv.org/abs/2311.08086)
## Unified Multi-task Models

#### Task-specific Outputs
- [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
- [You only look at once for real-time and generic multi-task](https://arxiv.org/abs/2310.01641)
- [CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation](https://arxiv.org/abs/2311.00987)
- [LiDAR-BEVMTN: Real-Time LiDAR Bird's-Eye View Multi-Task Perception Network for Autonomous Driving](https://arxiv.org/abs/2307.08850)
- [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2311.16038)

#### Unified Language Outputs
- [DRAMA: Joint Risk Localization and Captioning in Driving](https://arxiv.org/abs/2209.10767)
- [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/abs/2302.00673)
- [HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/abs/2309.05186)
- [Traffic Sign Interpretation in Real Road Scene](https://arxiv.org/abs/2311.10793)
- [Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/abs/2312.03661)
## Unified Multi-modal Models

#### LLM Functions as Sequence Modeling
- [A Language Agent for Autonomous Driving](https://arxiv.org/abs/2311.10813)
- [Empowering Autonomous Driving with Large Language Models: A Safety Perspective](https://arxiv.org/abs/2312.00812)
- [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)
- [Drive Like a Human: Rethinking Autonomous Driving with Large Language Models](https://arxiv.org/abs/2307.07162)
- [Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://arxiv.org/abs/2310.01957)
- [LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving](https://arxiv.org/abs/2310.03026)
- [DRIVEGPT4: INTERPRETABLE END-TO-END AUTONOMOUS DRIVING VIA LARGE LANGUAGE MODEL](https://arxiv.org/abs/2310.01412)
- [GPT-Driver: Learning to Drive with GPT](https://arxiv.org/abs/2310.01415)
- [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245)
#### Cross-modal Interaction in VLM
- [Language Prompt for Autonomous Driving](https://arxiv.org/abs/2309.04379)
- [Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving](https://arxiv.org/abs/2310.02251)
- [Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models](https://arxiv.org/abs/2310.17642)
- [Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/abs/2312.03661)
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
- [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/abs/2311.05332)

## Prompting Foundation Models

#### [Textual Prompt](#llm-functions-as-sequence-modeling)

#### Visual Prompt
- [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
- [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/abs/2311.05332)
- [Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving](https://arxiv.org/abs/2310.02251)
- [Talk2Car: Taking Control of Your Self-Driving Car](https://arxiv.org/abs/1909.10838)
- [Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?](https://arxiv.org/abs/2304.10224)
- [Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2310.04456)


#### Multi-step Prompt
- [OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data](https://arxiv.org/abs/2310.13398)
- [GPT-Driver: Learning to Drive with GPT](https://arxiv.org/abs/2310.01415)
- [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)
#### Task-specific Prompt
- [Multi-task learning with multi-query transformer for dense prediction](https://arxiv.org/abs/2205.14354)
- [Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection, Segmentation, and Depth Estimation](https://arxiv.org/abs/2304.00971)
- [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
- [Effective Adaptation in Multi-Task Co-Training for Unified Autonomous Driving](https://arxiv.org/abs/2209.08953)

#### Geometric Prompt
- [Beyond Empirical Windowing: An Attention-Based Approach for Trust Prediction in Autonomous Vehicles](https://arxiv.org/abs/2312.10209)
- [DRIVEGPT4: INTERPRETABLE END-TO-END AUTONOMOUS DRIVING VIA LARGE LANGUAGE MODEL](https://arxiv.org/abs/2310.01412)
#### Prompt Pool
- [I3DOD: Towards Incremental 3D Object Detection via Prompting](https://arxiv.org/abs/2308.12512)
## Towards Open-world Understanding

#### Label Shift

#### Domain Shift

## Efficient Transfer for Road Scenes

#### Knowledge Distillation

#### Instant Learning

## Continual Learning

#### Rehearsal-based methods

#### Architecture-based methods

## Learn to Interact

## Generative Foundation Models

## Closed-loop Driving Systems

## Interpretability

## Embodied Driving Agent

## World Model





